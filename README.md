# DS5983 Special Topics in Data Science - LLMs

## Projects

* **Project 1 (N-gram models):** Test various n-gram models on Reuters.
* **Project 2 (Transformer Architecture Implementation and Machine Translation):** Implement Transformer model architecture form scratch. Train the model on a machine translation task from German to English using the Multi30k dataset. Implement greedy decoding and beam search for translation.
* **Project 3 (Text Summarization with HuggingFace BART):**  Evaluated summarization of 3 pre-trained Hugging Face Transformer models BART, T5, Pegasus on SAMSum dataset. Improved BART modelâ€™s ROUGE score on testing set from 28.7 to 37.5 by fine-tuning with Mixed Precision.

### Syllabus

* Learn about different types of language models (statistical, neural).
* Understand the key components of large language models, their architecture, configuration, training procedures and optimization.    
* Understand the transformer architecture and its variants.
* Learn how to work with the HuggingFace transformer library.
* Become familiar with popular LLMs such as GPT, BERT, Flan-T5 and LLaMA.
* Implement, train, and fine-tune LLMs for various tasks, such as text genearation, summarization, and question answering.
* Evaluate and analyze the performance of LLMs using various evaluation metrics.
* Learn how to use LLMs as agents and general-purpose task solvers.
* Discuss the ethical and societal implications of using LLMs in different scenarios.
* Demonstrate creativity and innovation by applying LLMs to novel tasks or by tackling current challenges in the field.
