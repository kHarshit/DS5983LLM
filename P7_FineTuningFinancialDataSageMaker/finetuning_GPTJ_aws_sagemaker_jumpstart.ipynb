{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e960490f",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - Fine-tuning text generation model on domain specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2327e",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to [Amazon SageMaker Built-in Algorithms](https://sagemaker.readthedocs.io/en/stable/algorithms/index.html)! You can use SageMaker Built-in algorithms to solve many Machine Learning tasks through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html). You can also use these algorithms through one-click in SageMaker Studio via [JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html).\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for finetuning Foundation Models and deploying the trained model for inference. The Foundation models perform Text Generation task. It takes a text string as input and predicts next words in the sequence.\n",
    "\n",
    "* **How to run inference on a large language model without finetuning.**\n",
    "* **How to fine-tune a large language model on a domain specific dataset, and then run inference on the fine-tuned model. In particular, the example dataset we demonstrated is [publicly available SEC filing](https://www.sec.gov/edgar/searchedgar/companysearch) of Amazon from year 2021 to 2022. The expectation is that after fine-tuning, the model should be able to generate insightful text in financial domain.**\n",
    "* **We compare the inference result for GPT-J 6B before finetuning and after finetuning.**\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091e1f6",
   "metadata": {},
   "source": [
    "1. [Set up](#1.-Set-up)\n",
    "2. [Select text generation model](#2.-Select-text-generation-model)\n",
    "3. [Run inference on the pre-trained model without finetuning](#3.-Run-inference-on-the-pre-trained-model-without-finetuning)\n",
    "    * [Retrieve artifacts & deploy an endpoint](#3.1.-Retrieve-artifacts-&-deploy-an-endpoint)\n",
    "    * [Query endpoint and parse response](#3.2.-Query-endpoint-and-parse-response)\n",
    "    * [Clean up the endpoint](#3.3.-Clean-up-the-endpoint)\n",
    "4. [Finetune the pre-trained model on a custom dataset](#4.-Fine-tune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Set training parameters](#4.1.-Set-training-parameters)\n",
    "    * [Train with Automatic Model Tuning](#4.2.-Train-with-Automatic-Model-Tuning-([HPO]))\n",
    "    * [Start training](#4.3.-Start-training)\n",
    "    * [Extract training performance metrics](#4.4.-Extract-training-performance-metrics)\n",
    "    * [Deploy & run inference on the fine-tuned model](#4.5.-Deploy-&-run-inference-on-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007b31a",
   "metadata": {},
   "source": [
    "## 1. Set up\n",
    "Before executing the notebook, there are some initial steps required for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b943ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1051f6",
   "metadata": {},
   "source": [
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a3eb07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee983c64",
   "metadata": {},
   "source": [
    "## 2. Select text generation model\n",
    "\n",
    "You can continue with the default model or choose a different model from the dropdown generated upon running the next cell. A complete list of JumpStart fine-tuned models can also be accessed at [JumpStart Fine-Tuned Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aed6e2c",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7abed65-8156-42fa-a237-6837f4cd3df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Select a pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3797f787fe14eeea62928f9a790f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "\n",
    "filter_value = And(\"task == textgeneration1\", \"framework == huggingface\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "dropdown = Dropdown(\n",
    "    value=model_id,\n",
    "    options=text_generation_models,\n",
    "    description=\"Sagemaker Pre-Trained Text Generation Models:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select a pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e49a7",
   "metadata": {},
   "source": [
    "## 3. Run inference on the pre-trained model without finetuning\n",
    "\n",
    "Using SageMaker, we can directly perform inference on a pre-trained text generation model. For example, [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) is an open source 6 billion parameter model released by Eleuther AI. GPT-J 6B has been trained on a large corpus of text data ([the Pile](https://pile.eleuther.ai/) dataset) and is capable of performing various natural language processing tasks such as text generation, text classification, and text summarization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f9242",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve artifacts & deploy an endpoint\n",
    "\n",
    "To host the pre-trained model, we create an instance of [`sagemaker.jumpstart.model.JumpStartModel`](https://sagemaker.readthedocs.io/en/stable/overview.html#deploy-a-pre-trained-model-directly-to-a-sagemaker-endpoint) and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d022545d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = dropdown.value, \"3.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecbe672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-textgeneration1-gpt-j-6b' with wildcard version identifier '3.*'. You can pin to version '3.1.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "\n",
    "base_model_predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce1248-9fa9-4617-9dfa-d432c03ad804",
   "metadata": {},
   "source": [
    "### 3.2. Query endpoint and parse response\n",
    "The model takes a text string as input and predicts next words in the sequence. We use three of following input examples.\n",
    "\n",
    "1. `This Form 10-K report shows that`\n",
    "2. `We serve consumers through`\n",
    "3. `Our vision is`\n",
    "\n",
    "**The input examples are related to company's perforamnce in financial report. You will see the outputs from the model without finetuning are limited in providing insightful contents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c63f536-be9b-4e52-a464-d206998e7071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1) we are unable to predict the actual results of our\n",
      "business in any future period or state of our business, (2) our actual results may differ materially from those anticipated in our consolidated financial statements due to a number of factors, including, without limitation, our inability to predict the impact of federal or state tax law changes, changes in accounting rules, our ability to manage our capital, competition, our ability to generate sufficient operating income to make distributions to our stockholders, adverse fluctuations in\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our company was founded in the year 2014, and since then, we have made our presence in the global market by offering a wide range of Vat Compliant & Custom Printed Shoes, Shirts, T-shirts, Sweaters, etc. We manufacture a broad array of products for many different types of business like sports shoes, accessories, clothing, etc. to help you get your business. We provide our services at reasonable rates.\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All of our students should be able to attend a school that is safe, comfortable, and a great learning environment.\n",
      "\n",
      "Every child has the right to a great education and to grow to be a productive, contributing member of society.\n",
      "\n",
      "We recognize that school discipline is the responsibility of the school, and we strive to provide an environment where our students learn in a safe, positive, and healthy manner.\n",
      "\n",
      "We realize that all children learn in different ways, and we are committed\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "for quota_text in [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]:\n",
    "    payload = {\"inputs\": f\"{quota_text}:\",\"parameters\": parameters}\n",
    "    generated_texts = base_model_predictor.predict(payload)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print('-'*20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbad7",
   "metadata": {},
   "source": [
    "### 3.3. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f8225e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70950bf9",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the pre-trained model on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f6f8c",
   "metadata": {},
   "source": [
    "Fine-tuning refers to the process of taking a pre-trained language model and retraining it for a different but related task using specific data. This approach is also known as transfer learning, which involves transferring the knowledge learned from one task to another. Large language models (LLMs) like GPT-J 6B are trained on massive amounts of unlabeled data and can be fine-tuned on domain domain datasets, making the model perform better on that specific domain. \n",
    "\n",
    "We will use financial text from SEC filings to fine tune a LLM GPT-J 6B for financial applications. \n",
    "\n",
    "\n",
    "\n",
    "- **Input**: A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file.\n",
    "    - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "    - The number of files under train and validation (if provided) should equal to one.\n",
    "- **Output**: A trained model that can be deployed for inference.\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "---\n",
    "```\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "\n",
    "GENERAL\n",
    "\n",
    "Embracing Our Future ...\n",
    "```\n",
    "---\n",
    "SEC filings data of Amazon is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfe06b",
   "metadata": {},
   "source": [
    "### 4.1. Set training parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our text generation model. Here, we define parameters that need to be set for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "036bac37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://jumpstart-cache-prod-us-east-1/training-datasets/sec_data/train/\n"
     ]
    }
   ],
   "source": [
    "# Sample training data is available in this bucket\n",
    "data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-tg-train\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "print(training_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87e3a8bf-f0f0-43a1-8ab8-cd3172099318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5556620\n",
      "1114952\n",
      "6.51520703125\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "bucket = boto3.resource('s3').Bucket(data_bucket)\n",
    "for object in bucket.objects.filter(Prefix=data_prefix):\n",
    "  total_size += object.size\n",
    "  print(object.size)\n",
    "print(total_size/1000/1024) #mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5051d41",
   "metadata": {},
   "source": [
    "### 4.2. Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c271247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d2622",
   "metadata": {},
   "source": [
    "### 4.3. Start training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job. For algorithm specific hyper-parameters, we override default `JumpStartEstimator` values for `epoch` and `per_device_train_batch_size`. You can view the full list of hyperparameters via `tg_estimator.hyperparameters`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "973d923c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: hf-textgeneration1-gpt-j-6b-2024-05-09-01-53-02-406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-09 01:53:02 Starting - Starting the training job...\n",
      "2024-05-09 01:53:02 Pending - Training job waiting for capacity......\n",
      "2024-05-09 01:54:20 Pending - Preparing the instances for training...\n",
      "2024-05-09 01:54:59 Downloading - Downloading input data................................................................................................\n",
      "2024-05-09 02:10:58 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:00,249 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:00,286 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:00,296 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:00,298 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:01,493 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.23.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.12.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.10.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.5.0-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.2-py2.py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.8-py2.py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.23.0->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.12.0->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 4)) (4.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.23.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.23.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0->-r requirements.txt (line 4)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0->-r requirements.txt (line 4)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.12.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.23.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907839 sha256=db3b4b17f9a5c10244f78d2e7e3bb6481f43f460482de825d1c3d9fb78b13d1a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b4/a0/a9/4723ccba9b5790d90f40617f369a69c6dff729fa4b0aa6e131\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, deepspeed, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 datasets-2.12.0 deepspeed-0.10.3 peft-0.5.0 safetensors-0.3.3 sagemaker-jumpstart-huggingface-script-utilities-1.1.2 sagemaker-jumpstart-script-utilities-1.1.8 sagemaker-jumpstart-tabular-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,334 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,334 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,391 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,439 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,486 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,496 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"bf16\": \"False\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epoch\": \"3\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"20\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": \"True\",\n",
      "        \"gradient_accumulation_steps\": \"2\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"instruction_tuned\": \"False\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"6e-06\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"10\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"-1\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"8\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": \"1\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"train_from_scratch\": \"False\",\n",
      "        \"validation_split_ratio\": \"0.2\",\n",
      "        \"warmup_ratio\": \"0.1\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"hf-textgeneration1-gpt-j-6b-2024-05-09-01-53-02-406\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"auto_find_batch_size\":\"False\",\"bf16\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"3\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"True\",\"gradient_accumulation_steps\":\"2\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"False\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"10\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"-1\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"hf-textgeneration1-gpt-j-6b-2024-05-09-01-53-02-406\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--auto_find_batch_size\",\"False\",\"--bf16\",\"False\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epoch\",\"3\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"20\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--instruction_tuned\",\"False\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"6e-06\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"-1\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--save_steps\",\"500\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--train_from_scratch\",\"False\",\"--validation_split_ratio\",\"0.2\",\"--warmup_ratio\",\"0.1\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=True\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=False\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=6e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FROM_SCRATCH=False\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --auto_find_batch_size False --bf16 False --dataloader_drop_last False --dataloader_num_workers 0 --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 3 --eval_accumulation_steps None --eval_steps 20 --evaluation_strategy steps --fp16 True --gradient_accumulation_steps 2 --gradient_checkpointing True --instruction_tuned False --label_smoothing_factor 0 --learning_rate 6e-06 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 10 --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_input_length -1 --max_steps -1 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 8 --per_device_train_batch_size 4 --preprocessing_num_workers None --save_steps 500 --save_strategy steps --save_total_limit 1 --seed 10 --train_data_split_seed 0 --train_from_scratch False --validation_split_ratio 0.2 --warmup_ratio 0.1 --warmup_steps 0 --weight_decay 0.2\u001b[0m\n",
      "\u001b[34m2024-05-09 02:11:18,527 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(model_dir='/opt/ml/model', train=None, train_alt='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', hosts=['algo-1'], num_gpus=4, current_host='algo-1', pretrained_model='/opt/ml/input/data/model', peft_type='None', lora_r=64, lora_alpha=16, lora_dropout=0.0, bits=16, double_quant=True, quant_type='nf4', deepspeed=True, instruction_tuned='False', train_from_scratch='False', fp16='True', bf16='False', evaluation_strategy='steps', eval_steps=20, epoch=3, gradient_accumulation_steps=2, per_device_train_batch_size=4, per_device_eval_batch_size=8, logging_steps=10, warmup_ratio=0.1, learning_rate=6e-06, weight_decay=0.2, load_best_model_at_end='True', max_train_samples=-1, max_val_samples=-1, seed=10, max_input_length=-1, validation_split_ratio=0.2, train_data_split_seed=0, preprocessing_num_workers=None, max_steps=-1, gradient_checkpointing='True', early_stopping_patience=3, early_stopping_threshold=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_nan_inf_filter='True', save_strategy='steps', save_steps=500, save_total_limit=1, dataloader_drop_last='False', dataloader_num_workers=0, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_steps=0).\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /tmp. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Parameter 'instruction_tuned' is identified as 'False'. Domain adaption fine-tuning will be started.\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:11,941] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:14,010] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:14,011] [INFO] [runner.py:570:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py --deepspeed ds_config.json --model_name_or_path /tmp --train_file /opt/ml/input/data/train --do_train --output_dir /opt/ml/model --num_train_epochs 3 --gradient_accumulation_steps 2 --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --logging_steps 10 --warmup_ratio 0.1 --learning_rate 6e-06 --weight_decay 0.2 --seed 10 --max_input_length -1 --validation_split_ratio 0.2 --train_data_split_seed 0 --max_steps -1 --early_stopping_patience 3 --early_stopping_threshold 0.0 --adam_beta1 0.9 --adam_beta2 0.999 --max_grad_norm 1.0 --label_smoothing_factor 0.0 --logging_strategy steps --save_strategy steps --save_steps 500 --dataloader_num_workers 0 --lr_scheduler_type constant_with_warmup --warmup_steps 0 --evaluation_strategy steps --eval_steps 20 --lora_r 64 --lora_alpha 16 --lora_dropout 0.0 --bits 16 --quant_type nf4 --validation_file /opt/ml/input/data/validation --load_best_model_at_end --fp16 --gradient_checkpointing --save_total_limit 1 --double_quant\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:15,647] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:163:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:17,620] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:22,684] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:22,686] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:22,687] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:22,688] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:24,824] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:24,828] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:24,856] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:24,857] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:24,869] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:25 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=2,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=6e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/May09_02-14-22_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=10,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant_with_warmup,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.2,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:25 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:25 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:25 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,852 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1807] 2024-05-09 02:14:25,853 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2024-05-09 02:14:25,913 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:666] 2024-05-09 02:14:25,913 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2024-05-09 02:14:25,929 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:720] 2024-05-09 02:14:25,929 >> Model config GPTJConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:25,946] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2024-05-09 02:14:25,946 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2531] 2024-05-09 02:14:25,946 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1176] 2024-05-09 02:14:25,946 >> Instantiating GPTJForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1176] 2024-05-09 02:14:25,946 >> Instantiating GPTJForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2024-05-09 02:14:25,946 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2623] 2024-05-09 02:14:25,946 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:25,946] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2024-05-09 02:14:25,950 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:575] 2024-05-09 02:14:25,950 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:25,950] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:25,956] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:14:31,962] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 285, num_elems = 6.05B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.71s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:21<00:10, 10.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.86s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2024-05-09 02:14:58,583 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2024-05-09 02:14:58,583 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3190] 2024-05-09 02:14:58,583 >> All model checkpoint weights were used when initializing GPTJForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3198] 2024-05-09 02:14:58,583 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2024-05-09 02:14:58,586 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2839] 2024-05-09 02:14:58,586 >> Generation config file not found, using a generation config created from the model config.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 14004.35it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 2153.69it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mDataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 715.75it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 715.63it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 706.17it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - datasets.builder -   Found cached dataset text (/root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 654.18it/s]\u001b[0m\n",
      "\u001b[34mThe tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34m05/09/2024 02:14:58 - WARNING - sagemaker_jumpstart_huggingface_script_utilities.fine_tuning.data_preprocessor -   The tokenizer picked has a `model_max_length` (2048) larger than maximum input length cap 1024. Picking 1024 instead.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   6%|▌         | 6000/106367 [00:00<00:02, 47900.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 43139.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   6%|▌         | 6000/106367 [00:00<00:02, 43268.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 5000/106367 [00:00<00:02, 43065.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█▏        | 12000/106367 [00:00<00:01, 51750.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:02, 47014.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█▏        | 12000/106367 [00:00<00:02, 45908.60 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 11000/106367 [00:00<00:01, 48735.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 18000/106367 [00:00<00:01, 50131.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 48754.01 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 18000/106367 [00:00<00:01, 47682.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 17000/106367 [00:00<00:01, 50504.27 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 50009.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 50976.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 24000/106367 [00:00<00:01, 48450.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 23000/106367 [00:00<00:01, 50042.82 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|██▊       | 30000/106367 [00:00<00:01, 50207.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 29000/106367 [00:00<00:01, 51292.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 29000/106367 [00:00<00:02, 38358.90 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  30%|███       | 32000/106367 [00:00<00:01, 40706.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  35%|███▍      | 37000/106367 [00:00<00:01, 46023.83 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  35%|███▍      | 37000/106367 [00:00<00:01, 38897.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 38000/106367 [00:00<00:01, 44370.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|███▎      | 35000/106367 [00:00<00:01, 40141.49 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  40%|████      | 43000/106367 [00:00<00:01, 47423.11 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  41%|████▏     | 44000/106367 [00:00<00:01, 43487.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▉      | 42000/106367 [00:00<00:01, 44890.26 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  42%|████▏     | 45000/106367 [00:00<00:01, 46949.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  46%|████▌     | 49000/106367 [00:01<00:01, 48994.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|████▋     | 50000/106367 [00:01<00:01, 44768.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  46%|████▌     | 49000/106367 [00:01<00:01, 48041.66 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  48%|████▊     | 51000/106367 [00:01<00:01, 47121.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 55000/106367 [00:01<00:01, 50329.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 55000/106367 [00:01<00:01, 45677.36 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 55000/106367 [00:01<00:01, 48194.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▎    | 57000/106367 [00:01<00:01, 47799.38 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 41171.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 39642.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 38994.85 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 62000/106367 [00:01<00:01, 41481.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  65%|██████▍   | 69000/106367 [00:01<00:00, 46203.48 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 68000/106367 [00:01<00:00, 43526.08 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 68000/106367 [00:01<00:00, 42376.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  65%|██████▍   | 69000/106367 [00:01<00:00, 44848.79 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████▏  | 76000/106367 [00:01<00:00, 48823.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  70%|██████▉   | 74000/106367 [00:01<00:00, 46013.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████   | 75000/106367 [00:01<00:00, 45835.41 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████   | 75000/106367 [00:01<00:00, 45474.64 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▌  | 80000/106367 [00:01<00:00, 46697.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  77%|███████▋  | 82000/106367 [00:01<00:00, 48484.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|███████▌  | 81000/106367 [00:01<00:00, 46577.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|███████▌  | 81000/106367 [00:01<00:00, 46032.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 88000/106367 [00:01<00:00, 48891.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  83%|████████▎ | 88000/106367 [00:01<00:00, 50254.88 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 85000/106367 [00:01<00:00, 37205.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  84%|████████▎ | 89000/106367 [00:02<00:00, 39817.55 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  87%|████████▋ | 93000/106367 [00:02<00:00, 44890.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%|█████████ | 96000/106367 [00:02<00:00, 45622.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  88%|████████▊ | 94000/106367 [00:02<00:00, 40186.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 95000/106367 [00:02<00:00, 41725.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  93%|█████████▎| 99000/106367 [00:02<00:00, 45932.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  96%|█████████▌| 102000/106367 [00:02<00:00, 46224.30 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  94%|█████████▍| 100000/106367 [00:02<00:00, 41714.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  95%|█████████▍| 101000/106367 [00:02<00:00, 42613.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|█████████▊| 105000/106367 [00:02<00:00, 45824.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|█████████▉| 106000/106367 [00:02<00:00, 43079.86 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 106367/106367 [00:02<00:00, 43858.71 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 50570.24 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  37%|███▋      | 7000/18760 [00:00<00:00, 55180.95 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 46476.43 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 6000/18760 [00:00<00:00, 49058.39 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  69%|██████▉   | 13000/18760 [00:00<00:00, 50961.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▍  | 14000/18760 [00:00<00:00, 55728.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 12000/18760 [00:00<00:00, 32543.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 15000/18760 [00:00<00:00, 36626.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 18760/18760 [00:00<00:00, 50856.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  96%|█████████▌| 18000/18760 [00:00<00:00, 40168.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/106367 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12821.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12822.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12951.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   2%|▏         | 2000/106367 [00:00<00:08, 12893.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14297.67 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14199.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14384.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   4%|▍         | 4000/106367 [00:00<00:07, 14316.80 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14872.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14732.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14925.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   6%|▌         | 6000/106367 [00:00<00:06, 14841.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15136.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 14997.38 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15162.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   8%|▊         | 8000/106367 [00:00<00:06, 15071.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15483.87 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15343.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15492.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   9%|▉         | 10000/106367 [00:00<00:06, 15400.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15398.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15270.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15400.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█▏        | 12000/106367 [00:00<00:06, 15303.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15694.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15542.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15672.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  13%|█▎        | 14000/106367 [00:00<00:05, 15571.87 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16049.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 15904.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 16006.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  15%|█▌        | 16000/106367 [00:01<00:05, 15934.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15614.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15463.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15586.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  17%|█▋        | 18000/106367 [00:01<00:05, 15504.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16225.80 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16030.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16190.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  20%|█▉        | 21000/106367 [00:01<00:05, 16111.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15956.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15765.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15913.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  22%|██▏       | 23000/106367 [00:01<00:05, 15829.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:04, 16325.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:05, 16236.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:06, 13275.23 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  24%|██▎       | 25000/106367 [00:01<00:06, 13163.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 16051.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:04, 15958.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:05, 13823.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  25%|██▌       | 27000/106367 [00:01<00:05, 13713.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16274.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:04, 16172.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:05, 14583.05 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  27%|██▋       | 29000/106367 [00:01<00:05, 14465.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16178.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:01<00:04, 16086.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:02<00:05, 14973.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  29%|██▉       | 31000/106367 [00:02<00:05, 14840.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 15965.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 15868.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 15120.72 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  31%|███       | 33000/106367 [00:02<00:04, 14972.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16495.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 16400.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 15876.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  33%|███▎      | 35000/106367 [00:02<00:04, 15729.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16154.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 16056.91 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 15724.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  35%|███▍      | 37000/106367 [00:02<00:04, 15601.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16730.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16633.53 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16414.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  37%|███▋      | 39000/106367 [00:02<00:04, 16275.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16474.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:03, 16377.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:04, 16252.28 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  39%|███▊      | 41000/106367 [00:02<00:04, 16122.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16582.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16475.39 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16419.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  40%|████      | 43000/106367 [00:02<00:03, 16292.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16304.75 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16193.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16198.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  42%|████▏     | 45000/106367 [00:02<00:03, 16058.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16137.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:02<00:03, 16039.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:03<00:03, 16078.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  44%|████▍     | 47000/106367 [00:03<00:03, 15943.06 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 15998.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 15913.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 15987.92 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  46%|████▌     | 49000/106367 [00:03<00:03, 15842.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15638.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15537.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15635.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  48%|████▊     | 51000/106367 [00:03<00:03, 15484.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15079.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 14970.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 15068.97 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  50%|████▉     | 53000/106367 [00:03<00:03, 14936.99 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15762.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15664.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15760.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  52%|█████▏    | 55000/106367 [00:03<00:03, 15635.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15673.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15568.29 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15664.09 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  54%|█████▎    | 57000/106367 [00:03<00:03, 15542.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15871.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15772.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15882.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  56%|█████▋    | 60000/106367 [00:03<00:02, 15756.85 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15595.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:03<00:02, 15495.81 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:04<00:02, 15602.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  58%|█████▊    | 62000/106367 [00:04<00:02, 15462.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15508.11 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15392.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15497.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  60%|██████    | 64000/106367 [00:04<00:02, 15362.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15453.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15361.62 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15470.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  62%|██████▏   | 66000/106367 [00:04<00:02, 15342.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15533.90 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15446.66 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15569.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 68000/106367 [00:04<00:02, 15422.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15670.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15570.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15696.40 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  66%|██████▌   | 70000/106367 [00:04<00:02, 15549.60 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15333.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15274.73 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15377.35 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  68%|██████▊   | 72000/106367 [00:04<00:02, 15232.25 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 15968.39 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 15887.68 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 15991.89 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  70%|██████▉   | 74000/106367 [00:04<00:02, 15846.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15454.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15366.01 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15468.83 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  71%|███████▏  | 76000/106367 [00:04<00:01, 15316.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15707.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15626.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15717.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  74%|███████▍  | 79000/106367 [00:05<00:01, 15577.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15456.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15360.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15460.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  76%|███████▌  | 81000/106367 [00:05<00:01, 15314.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15933.56 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15823.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15943.13 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  78%|███████▊  | 83000/106367 [00:05<00:01, 15783.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15865.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15741.32 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15859.31 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  80%|███████▉  | 85000/106367 [00:05<00:01, 15716.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15867.21 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15744.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15874.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  82%|████████▏ | 87000/106367 [00:05<00:01, 15722.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15739.22 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15621.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15751.69 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  84%|████████▎ | 89000/106367 [00:05<00:01, 15599.71 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15698.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15595.18 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15712.20 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  86%|████████▌ | 91000/106367 [00:05<00:00, 15568.94 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15896.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15804.12 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:05<00:00, 15910.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  87%|████████▋ | 93000/106367 [00:06<00:00, 15756.43 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15661.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15566.03 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15664.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  89%|████████▉ | 95000/106367 [00:06<00:00, 15498.60 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16301.50 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16205.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16309.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  91%|█████████ | 97000/106367 [00:06<00:00, 16138.98 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14891.00 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14901.16 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14793.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  93%|█████████▎| 99000/106367 [00:06<00:00, 14764.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14151.86 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14169.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14057.57 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  95%|█████████▍| 101000/106367 [00:06<00:00, 14040.63 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14046.79 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 14068.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 13966.58 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  97%|█████████▋| 103000/106367 [00:06<00:00, 13936.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14443.70 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14459.54 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14353.44 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  99%|█████████▊| 105000/106367 [00:06<00:00, 14309.59 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:   0%|          | 0/18760 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14087.37 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 14063.17 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 13922.75 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  11%|█         | 2000/18760 [00:00<00:01, 13871.47 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14152.04 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 14158.74 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 13996.02 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  21%|██▏       | 4000/18760 [00:00<00:01, 13950.52 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14379.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14384.64 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14213.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  32%|███▏      | 6000/18760 [00:00<00:00, 14169.34 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14390.76 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14414.33 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14254.10 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  43%|████▎     | 8000/18760 [00:00<00:00, 14226.78 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14427.26 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14457.42 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14292.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  53%|█████▎    | 10000/18760 [00:00<00:00, 14251.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14495.77 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14532.30 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14369.84 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  64%|██████▍   | 12000/18760 [00:00<00:00, 14320.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14415.95 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14446.19 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14291.14 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  75%|███████▍  | 14000/18760 [00:00<00:00, 14231.88 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14221.41 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14275.55 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14119.93 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  85%|████████▌ | 16000/18760 [00:01<00:00, 14064.15 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14379.36 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14414.49 examples/s]\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14261.67 examples/s]\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-059144379f6d3fdd.arrow\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a87c9c7c46a999c0.arrow\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,843] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-059144379f6d3fdd.arrow\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a87c9c7c46a999c0.arrow\u001b[0m\n",
      "\u001b[34mGrouping texts in chunks of 1024:  96%|█████████▌| 18000/18760 [00:01<00:00, 14202.97 examples/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2024-05-09 02:15:09,885 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:621] 2024-05-09 02:15:09,885 >> Using cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,889] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,890] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,908] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-059144379f6d3fdd.arrow\u001b[0m\n",
      "\u001b[34m05/09/2024 02:15:09 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/text/default-8a1b24112504b085/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a87c9c7c46a999c0.arrow\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,975] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:09,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:11,118] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:11,122] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:11,123] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:11,133] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.268537998199463 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.313812017440796 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.33082389831543 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.33589482307434 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000006, betas=(0.900000, 0.999000), weight_decay=0.200000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,559] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,575] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,575] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,575] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,575] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,683] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,684] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.89 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,684] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.79 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,686] [INFO] [stage3.py:126:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,686] [INFO] [stage3.py:127:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,794] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,794] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,794] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.79 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 811008 in 114 params\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,920] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,920] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:42,921] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.8 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:43,031] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:43,032] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:43,032] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 33.8 GB, percent = 18.1%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:46,958] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:46,960] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:46,961] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 44.69 GB, percent = 23.9%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:47,141] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:47,143] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:47,143] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 47.64 GB, percent = 25.5%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,210] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,211] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,211] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.88 GB, percent = 34.8%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,346] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,347] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:49,347] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 67.48 GB, percent = 36.1%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:57,481] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:57,482] [INFO] [utils.py:804:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:57,482] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 133.05 GB, percent = 71.3%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:15:57,482] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,819] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,820] [INFO] [utils.py:804:see_memory_usage] MA 0.15 GB         Max_MA 0.92 GB         CA 1.55 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,820] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 144.45 GB, percent = 77.4%\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,821] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb72420ffa0>\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[6e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,821] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb782104400>\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,822] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   optimizer_params ............. {'lr': 6e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.2}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 6e-06, 'warmup_num_steps': 10}\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  4\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,823] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-05-09 02:16:00,824] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 6e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.2\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 6e-06, \n",
      "            \"warmup_num_steps\": 10\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2024-05-09 02:16:00,825 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2024-05-09 02:16:00,825 >>   Num examples = 1,030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2024-05-09 02:16:00,825 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2024-05-09 02:16:00,825 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2024-05-09 02:16:00,825 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2024-05-09 02:16:00,825 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2024-05-09 02:16:00,825 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1769] 2024-05-09 02:16:00,825 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1770] 2024-05-09 02:16:00,825 >>   Num examples = 1,030\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1771] 2024-05-09 02:16:00,825 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1772] 2024-05-09 02:16:00,825 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1773] 2024-05-09 02:16:00,825 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1774] 2024-05-09 02:16:00,825 >>   Gradient Accumulation steps = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1775] 2024-05-09 02:16:00,825 >>   Total optimization steps = 96\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2024-05-09 02:16:00,826 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1776] 2024-05-09 02:16:00,826 >>   Number of trainable parameters = 6,050,882,784\u001b[0m\n",
      "\u001b[34m0%|          | 0/96 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/96 [00:39<1:03:16, 39.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/96 [01:14<57:41, 36.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/96 [01:49<55:26, 35.77s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/96 [02:23<53:52, 35.13s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/96 [02:57<53:01, 34.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 6/96 [03:31<51:49, 34.55s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/96 [04:05<50:50, 34.28s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 8/96 [04:39<50:12, 34.24s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 9/96 [05:14<49:49, 34.36s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [05:47<48:56, 34.15s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6971, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 10/96 [05:47<48:56, 34.15s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 11/96 [06:21<48:20, 34.13s/it]\u001b[0m\n",
      "\u001b[34m12%|█▎        | 12/96 [06:55<47:24, 33.86s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 13/96 [07:29<46:56, 33.94s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 14/96 [08:03<46:21, 33.92s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 15/96 [08:36<45:40, 33.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 16/96 [09:10<45:13, 33.92s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 17/96 [09:44<44:24, 33.73s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 18/96 [10:18<44:00, 33.85s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 19/96 [10:52<43:29, 33.89s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:25<42:39, 33.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3914, 'learning_rate': 6e-06, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:25<42:39, 33.68s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:27:26,359 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:27:26,359 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:27:26,362 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:27:26,362 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:27:26,362 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:27:26,362 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.72s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:06<00:09,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.90s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:17<00:03,  3.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.41s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.2705078125, 'eval_runtime': 25.8903, 'eval_samples_per_second': 7.648, 'eval_steps_per_second': 0.27, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 20/96 [11:51<42:39, 33.68s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  3.41s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 21/96 [12:25<52:08, 41.72s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 22/96 [12:59<48:18, 39.17s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 23/96 [13:33<45:50, 37.67s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 24/96 [14:06<43:41, 36.40s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 25/96 [14:40<42:12, 35.66s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 26/96 [15:14<40:54, 35.06s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 27/96 [15:48<39:55, 34.71s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 28/96 [16:22<39:07, 34.52s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 29/96 [16:56<38:26, 34.43s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 30/96 [17:30<37:35, 34.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1307, 'learning_rate': 6e-06, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 30/96 [17:30<37:35, 34.18s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 31/96 [18:03<36:43, 33.90s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 32/96 [18:37<36:08, 33.88s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 33/96 [19:09<35:01, 33.36s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 34/96 [19:42<34:23, 33.28s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 35/96 [20:17<34:24, 33.85s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 36/96 [20:52<34:07, 34.12s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 37/96 [21:26<33:25, 33.99s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 38/96 [21:59<32:48, 33.93s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 39/96 [22:32<31:53, 33.57s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:06<31:28, 33.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8149, 'learning_rate': 6e-06, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:06<31:28, 33.73s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:39:07,621 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:39:07,621 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:39:07,621 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:39:07,621 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:39:07,621 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:39:07,621 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.82s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.97s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.34s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.78076171875, 'eval_runtime': 25.4734, 'eval_samples_per_second': 7.773, 'eval_steps_per_second': 0.275, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 40/96 [23:32<31:28, 33.73s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:22<00:00,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 41/96 [24:05<37:55, 41.37s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 42/96 [24:40<35:21, 39.28s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 43/96 [25:13<33:05, 37.47s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 44/96 [25:46<31:23, 36.22s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 45/96 [26:20<30:10, 35.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 46/96 [26:54<29:15, 35.12s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 47/96 [27:28<28:24, 34.78s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 48/96 [28:03<27:41, 34.62s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 49/96 [28:37<27:00, 34.49s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [29:11<26:16, 34.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6572, 'learning_rate': 6e-06, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [29:11<26:16, 34.28s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 51/96 [29:45<25:41, 34.25s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 52/96 [30:19<25:05, 34.21s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 53/96 [30:53<24:23, 34.04s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 54/96 [31:27<23:55, 34.17s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 55/96 [32:01<23:19, 34.13s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 56/96 [32:35<22:37, 33.93s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 57/96 [33:08<22:02, 33.92s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 58/96 [33:43<21:31, 33.99s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 59/96 [34:16<20:51, 33.81s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [34:50<20:17, 33.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5648, 'learning_rate': 6e-06, 'epoch': 1.85}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [34:50<20:17, 33.81s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:50:51,181 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 02:50:51,181 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:50:51,188 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 02:50:51,188 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:50:51,189 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 02:50:51,189 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.81s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.97s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.35s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.53955078125, 'eval_runtime': 25.4913, 'eval_samples_per_second': 7.767, 'eval_steps_per_second': 0.275, 'epoch': 1.85}\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 60/96 [35:15<20:17, 33.81s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:22<00:00,  3.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 61/96 [35:49<24:10, 41.44s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 62/96 [36:23<22:11, 39.16s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 63/96 [36:57<20:42, 37.65s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 64/96 [37:30<19:18, 36.19s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 65/96 [38:02<18:06, 35.05s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 66/96 [38:35<17:14, 34.47s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 67/96 [39:09<16:32, 34.24s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 68/96 [39:43<16:00, 34.31s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 69/96 [40:17<15:22, 34.16s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 70/96 [40:51<14:42, 33.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4614, 'learning_rate': 6e-06, 'epoch': 2.15}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 70/96 [40:51<14:42, 33.93s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 71/96 [41:25<14:09, 33.98s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 72/96 [41:59<13:34, 33.93s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 73/96 [42:32<12:54, 33.68s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 74/96 [43:05<12:17, 33.50s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 75/96 [43:38<11:44, 33.53s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 76/96 [44:12<11:13, 33.67s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 77/96 [44:46<10:37, 33.55s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 78/96 [45:19<10:02, 33.45s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 79/96 [45:53<09:29, 33.51s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:26<08:57, 33.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3456, 'learning_rate': 6e-06, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:26<08:57, 33.62s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 03:02:27,755 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 03:02:27,755 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 03:02:27,756 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 03:02:27,756 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 03:02:27,756 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 03:02:27,756 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.82s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.97s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.20s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:18<00:03,  3.34s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.43408203125, 'eval_runtime': 25.4547, 'eval_samples_per_second': 7.779, 'eval_steps_per_second': 0.275, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [46:52<08:57, 33.62s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  3.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 81/96 [47:24<10:13, 40.93s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 82/96 [47:58<09:04, 38.88s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 83/96 [48:32<08:04, 37.23s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 84/96 [49:05<07:11, 35.94s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 85/96 [49:38<06:25, 35.09s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 86/96 [50:11<05:44, 34.47s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 87/96 [50:44<05:07, 34.16s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 88/96 [51:17<04:30, 33.80s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 89/96 [51:51<03:56, 33.74s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [52:24<03:21, 33.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3468, 'learning_rate': 6e-06, 'epoch': 2.77}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [52:24<03:21, 33.60s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 91/96 [52:58<02:47, 33.54s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 92/96 [53:31<02:14, 33.56s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 93/96 [54:05<01:40, 33.55s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 94/96 [54:38<01:07, 33.59s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 95/96 [55:12<00:33, 33.58s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:45<00:00, 33.53s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2024-05-09 03:11:46,712 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2024-05-09 03:11:46,712 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 3345.8875, 'train_samples_per_second': 0.924, 'train_steps_per_second': 0.029, 'train_loss': 0.79360032081604, 'epoch': 2.95}\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:45<00:00, 33.53s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [55:45<00:00, 34.85s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       2.95\u001b[0m\n",
      "\u001b[34mtrain_loss               =     0.7936\u001b[0m\n",
      "\u001b[34mtrain_runtime            = 0:55:45.88\u001b[0m\n",
      "\u001b[34mtrain_samples            =       1030\u001b[0m\n",
      "\u001b[34mtrain_samples_per_second =      0.924\u001b[0m\n",
      "\u001b[34mtrain_steps_per_second   =      0.029\u001b[0m\n",
      "\u001b[34m05/09/2024 03:11:46 - INFO - __main__ -   Start Evaluation.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 03:11:46,726 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2024-05-09 03:11:46,726 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 03:11:46,727 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2024-05-09 03:11:46,727 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 03:11:46,727 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2024-05-09 03:11:46,727 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:10,  2.55s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.94s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.16s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:17<00:03,  3.30s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.11s/it]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\n",
      "  epoch                   =       2.95\n",
      "  eval_loss               =     0.3635\n",
      "  eval_runtime            = 0:00:25.19\n",
      "  eval_samples            =        198\n",
      "  eval_samples_per_second =      7.859\n",
      "  eval_steps_per_second   =      0.278\n",
      "  perplexity              =     1.4384\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2024-05-09 03:12:11,924 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2024-05-09 03:12:11,924 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2024-05-09 03:12:11,925 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2024-05-09 03:12:11,925 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2024-05-09 03:12:11,925 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2024-05-09 03:12:11,925 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2024-05-09 03:12:12,104 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2024-05-09 03:12:12,104 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2024-05-09 03:12:12,105 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2024-05-09 03:12:12,105 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2024-05-09 03:12:12,105 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2024-05-09 03:12:12,105 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:24,457] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step96 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:24,457] [INFO] [engine.py:3486:save_16bit_model] Saving model weights to /opt/ml/model/pytorch_model.bin, tag: global_step96\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:24,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:30,307] [INFO] [launch.py:347:main] Process 133 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:31,308] [INFO] [launch.py:347:main] Process 132 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:32,310] [INFO] [launch.py:347:main] Process 134 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:45,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:45,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step96 is ready now!\u001b[0m\n",
      "\u001b[34m[2024-05-09 03:12:53,332] [INFO] [launch.py:347:main] Process 131 exits successfully.\u001b[0m\n",
      "\u001b[34mINFO:root:Convert: [2/2] -- Took: 0:00:26.563509 s\u001b[0m\n",
      "\u001b[34m2024-05-09 03:13:25,171 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-09 03:13:25,171 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-09 03:13:25,171 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-09 03:13:32 Uploading - Uploading generated training model\n",
      "2024-05-09 03:14:28 Completed - Training job completed\n",
      "Training seconds: 4770\n",
      "Billable seconds: 4770\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters={\n",
    "        \"epoch\": \"3\",\n",
    "        \"per_device_train_batch_size\": \"4\",\n",
    "    },\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed581b",
   "metadata": {},
   "source": [
    "### 4.4. Extract training performance metrics\n",
    "***\n",
    "Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce268cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1740.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2100.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2460.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2820.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval:loss</td>\n",
       "      <td>1.270508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name     value\n",
       "0        0.0  train:loss  1.697100\n",
       "1      360.0  train:loss  1.391400\n",
       "2      720.0  train:loss  1.130700\n",
       "3     1080.0  train:loss  0.814900\n",
       "4     1440.0  train:loss  0.657200\n",
       "5     1740.0  train:loss  0.564800\n",
       "6     2100.0  train:loss  0.461400\n",
       "7     2460.0  train:loss  0.345600\n",
       "8     2820.0  train:loss  0.346800\n",
       "9        0.0   eval:loss  1.270508"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ae6f5-2505-429c-9e47-5930292784f6",
   "metadata": {},
   "source": [
    "```\n",
    "***** eval metrics *****\n",
    "  epoch                   =       2.95\n",
    "  eval_loss               =     0.3635\n",
    "  eval_runtime            = 0:00:25.19\n",
    "  eval_samples            =        198\n",
    "  eval_samples_per_second =      7.859\n",
    "  eval_steps_per_second   =      0.278\n",
    "  perplexity              =     1.4384\n",
    " ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d20f9",
   "metadata": {},
   "source": [
    "## 4.5. Deploy & run inference on the fine-tuned model\n",
    "***\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an input sentence. We follow the same steps as in [3. Run inference on the pre-trained model without finetuning](#3.-Run-inference-on-the-pre-trained-model-without-finetuning). We start by retrieving the artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `tg_estimator` that we fine-tuned.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce738168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: hf-textgeneration1-gpt-j-6b-2024-05-09-03-26-40-668\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-huggingface-textgener-2024-05-09-03-26-40-668\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-huggingface-textgener-2024-05-09-03-26-40-668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "endpoint_name_after_finetune = name_from_base(f\"jumpstart-example-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=model.instance_type,\n",
    "    image_uri=model.image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541ccf7-d26e-4088-b52a-8fd0bf93bafc",
   "metadata": {},
   "source": [
    "Next, we query the finetuned model using the same set of examples above, parse the response and print the predictions. The outputs from fine-tune model are generated as below. We can see that after being fine-tuned, the model can generate more insightful contents related to financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6aa706-99d1-47c2-b712-c279a89eb63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)The following table provides information about our agreements, includinglegal provisions, regarding lock-up and conversion of certain types ofinvestments (in thousands, except percentages):The following table provides information about collateralization of certainof our liabilities (in thousands):Amazon.com Int’l Sales, Inc.  AMAZON.COM, INC.NOTES TO CONSOLIDATED FINANCIAL STATEMENTS-(Continued)The following table summarizes contractual maturities of our\n",
      "\n",
      "\n",
      " Amazon Web Services, which provides technology infrastructure to start-ups and enterprises of all sizes, and to developers building all types of applications; Amazon Books, which offers customers access to over a million new, used, and out-of-print books; Amazon Game Store, which offers quality games for the Amazon.com platform; Amazon Elastic Compute Cloud (Amazon EC2), which provides on-demand compute, storage, database, and other service capabilities for developers and enterprises of all sizes; and digital\n",
      "\n",
      "\n",
      " To be the world’s best media and entertainment company, recognized for the passion, creativity and commitment of our people, the quality and innovation of our content and services, and the reliability, speed and innovation of our technology.At Amazon.com,we fulfill a vision of earthship where our customers can find and discover anything they might want to buy online. We are guided by four principles: customer obsession rather than competitor focus, passion for invention, commitment to operational excellence, and long-term\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "for quota_text in [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is \",\n",
    "]:\n",
    "    payload = {\"inputs\": f\"{quota_text}:\", \"parameters\": parameters}\n",
    "    generated_texts = finetuned_predictor.predict(payload)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10989f-71f7-4f0e-ba5c-5dd7b375cf69",
   "metadata": {},
   "source": [
    "We compare the outputs between the model before fine-tuning and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "280a1d7c-4236-4dd5-aeab-912ff582d498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input example</th>\n",
       "      <th>Output before finetuning</th>\n",
       "      <th>Output after finetuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Form 10-K report shows that</td>\n",
       "      <td>(1) we are unable to predict the actual resul...</td>\n",
       "      <td>(1)The following table provides information ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We serve consumers through</td>\n",
       "      <td>\\n\\nOur company was founded in the year 2014, ...</td>\n",
       "      <td>Amazon Web Services, which provides technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our vision is</td>\n",
       "      <td>\\n\\nAll of our students should be able to atte...</td>\n",
       "      <td>To be the world’s best media and entertainmen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Input example  \\\n",
       "0  This Form 10-K report shows that   \n",
       "1        We serve consumers through   \n",
       "2                     Our vision is   \n",
       "\n",
       "                            Output before finetuning  \\\n",
       "0   (1) we are unable to predict the actual resul...   \n",
       "1  \\n\\nOur company was founded in the year 2014, ...   \n",
       "2  \\n\\nAll of our students should be able to atte...   \n",
       "\n",
       "                             Output after finetuning  \n",
       "0  (1)The following table provides information ab...  \n",
       "1   Amazon Web Services, which provides technolog...  \n",
       "2   To be the world’s best media and entertainmen...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Input example\": [\n",
    "            \"This Form 10-K report shows that\",\n",
    "            \"We serve consumers through\",\n",
    "            \"Our vision is\",\n",
    "        ],\n",
    "        \"Output before finetuning\": res_gpt_before_finetune,\n",
    "        \"Output after finetuning\": res_gpt_finetune,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dae9864-90af-496d-b217-4713a6da73fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (1) our Code of Business\\nEthics and Conduct (\"Code\") and compliance certification page; (2) the\\nrisk factors of our Annual Report on Form 10-K for the year ended December\\n31, 2017; (3) the restatement of our Annual Report on Form 10-K for the year\\nended December 31, 2016; (4) our Proxy Statement for our Annual Meeting of\\nShareholders, to be filed with the SEC in connection with our 2019 Annual\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quota_text = \"The SEC filing data consists of\"\n",
    "payload = {\"inputs\": f\"{quota_text}:\", \"parameters\": parameters}\n",
    "generated_text = finetuned_predictor.predict(payload)[0][\"generated_text\"]\n",
    "generated_text   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d9168",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we clean up the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49f98c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: hf-textgeneration1-gpt-j-6b-2024-05-09-03-26-40-668\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-example-huggingface-textgener-2024-05-09-03-26-40-668\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-example-huggingface-textgener-2024-05-09-03-26-40-668\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818b208-0d30-4e24-9d3a-35d6d503f019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.r5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
