{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a4cd103",
      "metadata": {
        "id": "3a4cd103"
      },
      "outputs": [],
      "source": [
        "# DS5983 project 2: instructor: Roi Yehoshua (due Feb 10)\n",
        "# Date: January 2024\n",
        "# MIT License\n",
        "\n",
        "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62a5dee5",
      "metadata": {
        "id": "62a5dee5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b17027ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17027ed",
        "outputId": "62cd5391-7677-42a2-b0c6-b812ac7bb91a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(42)  # For reproducibility\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffad93f9",
      "metadata": {
        "id": "ffad93f9"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "$$\n",
        "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
        "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf156177",
      "metadata": {
        "id": "bf156177"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"The multi-head attention module\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure the dimension of the model is divisible by the number of heads.\n",
        "        # This is necessary to equally divide the embedding dimension across heads.\n",
        "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "\n",
        "        self.d_model = d_model           # Total dimension of the model\n",
        "        self.num_heads = num_heads       # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # 1. Calculate attention scores with scaling\n",
        "        # 2. Apply mask (if provided) by setting masked positions to a large negative value\n",
        "        # 3. Apply softmax to attention scores to get probabilities\n",
        "        # 4. Return the weighted sum of values based on attention probabilities\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        prob_attn = F.softmax(scores, dim=-1)\n",
        "        output =  torch.matmul(prob_attn, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
        "        # to prepare for multi-head attention processing\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
        "        # [batch_size, seq_length, d_model]\n",
        "        batch_size, num_heads, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # 1. Linearly project the queries, keys, and values, and then split them into heads\n",
        "        # 2. Apply scaled dot-product attention for each head\n",
        "        # 3. Concatenate the heads' outputs and apply the final linear projection\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # apply same mask to all `num_heads` heads.\n",
        "        # if mask is not None:\n",
        "        #     mask = mask.unsqueeze(1)#.repeat(1, self.num_heads, 1, 1)\n",
        "\n",
        "        # print(f'mask shape: {mask.shape}')\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        # print(f'attn_output shape: {attn_output.shape}')\n",
        "        attn_output = self.combine_heads(attn_output)\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b13867",
      "metadata": {
        "id": "70b13867"
      },
      "source": [
        "### Feed-Forward NN\n",
        "\n",
        "$$\n",
        "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d5ab551",
      "metadata": {
        "id": "5d5ab551"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4034ef0a",
      "metadata": {
        "id": "4034ef0a"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "$$\n",
        "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
        "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bfca835c",
      "metadata": {
        "id": "bfca835c"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding module using sinusoidal functions of different frequencies\n",
        "    for each dimension of the encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
        "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the division term used in the formulas for sin and cos functions.\n",
        "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
        "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
        "        # multiplying the position by the division term, creating a pattern where each position has\n",
        "        # a unique sinusoidal encoding.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
        "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
        "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor x.\n",
        "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
        "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
        "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e2475b",
      "metadata": {
        "id": "96e2475b"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "000313c6",
      "metadata": {
        "id": "000313c6"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # Self-attention sublayer with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Feedforward sublayer with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76215e02",
      "metadata": {
        "id": "76215e02"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee27007e",
      "metadata": {
        "id": "ee27007e"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # Self-attention sublayer with residual connection and layer normalization\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = x + self.dropout(self_attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Cross-attention sublayer with residual connection and layer normalization\n",
        "        # query=decoder input, key and value=encoder output\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = x + self.dropout(cross_attn_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        # Feedforward sublayer with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad86614",
      "metadata": {
        "id": "2ad86614"
      },
      "source": [
        "### The Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31af1f2a",
      "metadata": {
        "id": "31af1f2a"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers for source and target\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialization\n",
        "        self.init_weights()\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_source_mask(self, src):\n",
        "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n",
        "        # Source padding mask\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(2) adds a dimension for the attention scores\n",
        "        # This mask can be broadcasted across the src_len dimension of the attention scores,\n",
        "        # effectively masking out specific tokens across all heads and all positions in the sequence.\n",
        "        return src_mask\n",
        "\n",
        "    def create_target_mask(self, tgt):\n",
        "        # Target padding mask\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(3) adds a dimension for the attention scores\n",
        "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n",
        "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
        "\n",
        "        # Target no-peak mask\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src):\n",
        "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
        "        \"\"\"\n",
        "        src_mask = self.create_source_mask(src)\n",
        "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
        "\n",
        "        # Pass through each layer in the encoder\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "        return src, src_mask\n",
        "\n",
        "    def decode(self, tgt, memory, src_mask):\n",
        "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
        "        \"\"\"\n",
        "        tgt_mask = self.create_target_mask(tgt)\n",
        "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
        "\n",
        "        # Pass through each layer in the decoder\n",
        "        for layer in self.decoder:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.out(tgt)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Encode the source sequence\n",
        "        memory, src_mask = self.encode(src)\n",
        "\n",
        "        # Decode the target sequence\n",
        "        output = self.decode(tgt, memory, src_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "11a3b60d",
      "metadata": {
        "id": "11a3b60d"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = 5000  # Size of source vocabulary\n",
        "tgt_vocab_size = 5000  # Size of target vocabulary\n",
        "d_model = 512          # Embedding dimension\n",
        "N = 6                  # Number of encoder and decoder layers\n",
        "num_heads = 8          # Number of attention heads\n",
        "d_ff = 2048            # Dimension of feed forward networks\n",
        "max_seq_length = 100   # Maximum sequence length\n",
        "dropout = 0.1          # Dropout rate\n",
        "pad_idx = 0            # Index of the padding token\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fab4a81d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab4a81d",
        "outputId": "c0961b50-12e0-4944-c952-35c9895aaf77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_embedding): Embedding(5000, 512)\n",
              "  (tgt_embedding): Embedding(5000, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): ModuleList(\n",
              "    (0-5): 6 x DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=5000, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050f7248",
      "metadata": {
        "id": "050f7248"
      },
      "source": [
        "### Testing on Random Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "149dadd1",
      "metadata": {
        "id": "149dadd1"
      },
      "outputs": [],
      "source": [
        "# Generate random sample data\n",
        "torch.manual_seed(42)\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5b1c56",
      "metadata": {
        "id": "2d5b1c56"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "45583975",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45583975",
        "outputId": "243dc748-bbae-4b96-c3eb-7d02cde3091c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([990], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Generate the next token using the first token in the first target tensor\n",
        "model.eval()\n",
        "\n",
        "memory, src_mask = model.encode(src_data[:1, :])\n",
        "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
        "y = output.view(-1, tgt_vocab_size).argmax(-1)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b7e1c5",
      "metadata": {
        "id": "18b7e1c5"
      },
      "source": [
        "If your code is correct, you should get tensor([990])."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a314148e",
      "metadata": {
        "id": "a314148e"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afd8ff3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afd8ff3",
        "outputId": "aeb255b3-7b47-4c61-a279-94fa090c2ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.605189323425293\n",
            "Epoch: 2, Loss: 8.501506805419922\n",
            "Epoch: 3, Loss: 8.371408462524414\n",
            "Epoch: 4, Loss: 8.296951293945312\n",
            "Epoch: 5, Loss: 8.23849868774414\n",
            "Epoch: 6, Loss: 8.192171096801758\n",
            "Epoch: 7, Loss: 8.164843559265137\n",
            "Epoch: 8, Loss: 8.142240524291992\n",
            "Epoch: 9, Loss: 8.130331993103027\n",
            "Epoch: 10, Loss: 8.121999740600586\n"
          ]
        }
      ],
      "source": [
        "# Train the model for 10 epochs\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "model.train()\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src_data, tgt_data[:, :-1])\n",
        "\n",
        "    # tgt_data is of shape [batch_size, tgt_len]\n",
        "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
        "    loss = criterion(output, tgt)\n",
        "\n",
        "    loss.backward()\n",
        "    # nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f36f48",
      "metadata": {
        "id": "42f36f48"
      },
      "source": [
        "You should see the loss decreasing from around 8.6 to 8.1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71caa7ed",
      "metadata": {
        "id": "71caa7ed"
      },
      "source": [
        "### Machine Translation Example\n",
        "\n",
        "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. <br>\n",
        "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a9a17e60",
      "metadata": {
        "id": "a9a17e60"
      },
      "outputs": [],
      "source": [
        "# To run this example make sure you have the following packages installed:\n",
        "# NOTE: Reload Colab session after installing\n",
        "%pip install spacy torchtext portalocker --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38e67f4",
      "metadata": {
        "id": "c38e67f4"
      },
      "source": [
        "#### Define Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "08f20abd",
      "metadata": {
        "id": "08f20abd"
      },
      "outputs": [],
      "source": [
        "# Load spacy models for tokenization\n",
        "try:\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download de_core_news_sm\")\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer, language):\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer(data_sample[language])\n",
        "\n",
        "tokenizer_de = get_tokenizer(tokenize_de)\n",
        "tokenizer_en = get_tokenizer(tokenize_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1c9820",
      "metadata": {
        "id": "0e1c9820"
      },
      "source": [
        "#### Build Vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7474b5e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7474b5e6",
        "outputId": "7348bb9f-4793-475f-8078-b23e1be90329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0),\n",
        "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1),\n",
        "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914ca38f",
      "metadata": {
        "id": "914ca38f"
      },
      "source": [
        "#### Create the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4c862170",
      "metadata": {
        "id": "4c862170"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters for the training process\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa58a6e",
      "metadata": {
        "id": "0aa58a6e"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3ca10d94",
      "metadata": {
        "id": "3ca10d94"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_data_iter):\n",
        "    data = []\n",
        "    for raw_src, raw_tgt in raw_data_iter:\n",
        "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
        "        data.append((src_tensor, tgt_tensor))\n",
        "    return data\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
        "train_data = data_process(train_data)\n",
        "valid_data = data_process(valid_data)\n",
        "#test_data = data_process(test_data)\n",
        "# The test set of Multi30k is corrupted\n",
        "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1c2f452a",
      "metadata": {
        "id": "1c2f452a"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch):\n",
        "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
        "    to each sequence and padding all sequences to the same length.\n",
        "\n",
        "    Parameters:\n",
        "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
        "      containing the source sequence tensor and the target sequence tensor.\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    # Iterate over each source-target pair in the provided batch\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n",
        "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n",
        "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n",
        "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
        "\n",
        "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
        "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
        "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2cbd8431",
      "metadata": {
        "id": "2cbd8431"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, grad_clip):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch over the given dataset.\n",
        "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
        "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - iterator (Iterable): An iterable object that returns batches of data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
        "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
        "    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode.\n",
        "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Enumerate over the data iterator to get batches\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model.\n",
        "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Reshape the output and target tensors to compute loss.\n",
        "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
        "\n",
        "        # tgt is of shape [batch_size, tgt_len]\n",
        "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "\n",
        "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss, perform backpropagation, and update model parameters\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Compute average loss per batch for the current epoch\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "31d18844",
      "metadata": {
        "id": "31d18844"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "    This function is similar to the training loop, but without the backward pass and parameter updates. I\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0287e5dd",
      "metadata": {
        "id": "0287e5dd"
      },
      "source": [
        "#### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02463b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d02463b8",
        "outputId": "e57c3d63-9eb8-4e07-bf81-08e0470f82c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.693\n",
            "\tVal Loss: 4.995\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.877\n",
            "\tVal Loss: 4.790\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.684\n",
            "\tVal Loss: 4.583\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.396\n",
            "\tVal Loss: 4.228\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.080\n",
            "\tVal Loss: 3.989\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.876\n",
            "\tVal Loss: 3.853\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.732\n",
            "\tVal Loss: 3.726\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.603\n",
            "\tVal Loss: 3.668\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.508\n",
            "\tVal Loss: 3.581\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.430\n",
            "\tVal Loss: 3.543\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.363\n",
            "\tVal Loss: 3.486\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.301\n",
            "\tVal Loss: 3.454\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.235\n",
            "\tVal Loss: 3.403\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.172\n",
            "\tVal Loss: 3.359\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.108\n",
            "\tVal Loss: 3.308\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.053\n",
            "\tVal Loss: 3.258\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 2.996\n",
            "\tVal Loss: 3.230\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.944\n",
            "\tVal Loss: 3.218\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.897\n",
            "\tVal Loss: 3.174\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.852\n",
            "\tVal Loss: 3.141\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a844bb",
      "metadata": {
        "id": "64a844bb"
      },
      "source": [
        "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save trained model\n",
        "torch.save(model.state_dict(), 'transformer_model.pt')"
      ],
      "metadata": {
        "id": "Y8AgPDre_fa-"
      },
      "id": "Y8AgPDre_fa-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITSyy_N2_tMk",
        "outputId": "ed3d554f-98c2-4846-8928-1183bb722681"
      },
      "id": "ITSyy_N2_tMk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp transformer_model.pt /content/drive/MyDrive/transformer_model.pt"
      ],
      "metadata": {
        "id": "uDhMYRzrAHEN"
      },
      "id": "uDhMYRzrAHEN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "817a7300",
      "metadata": {
        "id": "817a7300"
      },
      "source": [
        "#### Translating a Sample Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0b1bd36c",
      "metadata": {
        "id": "0b1bd36c"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_width=5):\n",
        "    model.eval()\n",
        "    tokens = tokenizer_de(sentence)\n",
        "    src_indexes = [vocab_src['<bos>']] + [vocab_src[token] for token in tokens] + [vocab_src['<eos>']]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    # Initialize beams as a list of tuples (score, [token indexes list])\n",
        "    beams = [(0, [vocab_tgt['<bos>']])]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for beam in beams:\n",
        "            trg_tensor = torch.LongTensor(beam[1]).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model.decode(trg_tensor, memory, src_mask)\n",
        "                output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "            # Consider the last token for further expansion\n",
        "            log_probs, next_tokens = output[0, -1].topk(beam_width)\n",
        "\n",
        "            for log_prob, next_token in zip(log_probs, next_tokens):\n",
        "                candidate = (beam[0] + log_prob.item(), beam[1] + [next_token.item()])\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Order all candidates by score\n",
        "        all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "        # Select the top beam_width candidates\n",
        "        beams = all_candidates[:beam_width]\n",
        "\n",
        "        # Break if the top beam candidate ends with <eos>\n",
        "        if beams[0][1][-1] == vocab_tgt['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Choose the best beam after completion\n",
        "    best_beam = beams[0][1]\n",
        "\n",
        "    # Convert the best beam's token indices to text\n",
        "    trg_tokens = [vocab_tgt.lookup_token(i) for i in best_beam]\n",
        "    translated_sentence = ' '.join(trg_tokens[1:])  # Skip <bos>\n",
        "\n",
        "    return translated_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438aff1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "438aff1f",
        "outputId": "2e4bc658-af51-4c78-830e-c2a93cd6d73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A young boy is playing in a pool . <eos>\n",
            "Translated sentence: A man in a hat and a woman in a black dress are sitting on a bench . <eos>\n",
            "Translated sentence: A crowd of people are working together . <eos>\n",
            "Translated sentence: Construction workers are working on a street . <eos>\n"
          ]
        }
      ],
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6be352",
      "metadata": {
        "id": "ec6be352"
      },
      "source": [
        "You should get a translation similar to the reference after 20 epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ],
      "metadata": {
        "id": "rUH9ep7xHTXk"
      },
      "id": "rUH9ep7xHTXk"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_beam_search(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_width=5):\n",
        "    model.eval()\n",
        "    tokens = tokenizer_de(sentence)  # Replace tokenizer_de with actual tokenization\n",
        "    src_indexes = [vocab_src['<bos>']] + [vocab_src[token] for token in tokens] + [vocab_src['<eos>']]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        memory, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    beams = [(0, [vocab_tgt['<bos>']])]  # Initialize beams with score and starting token\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for score, trg_indexes in beams:\n",
        "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model.decode(trg_tensor, memory, src_mask)\n",
        "                output = F.log_softmax(output, dim=-1)\n",
        "\n",
        "            log_probs, tokens = output[:, -1, :].topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                next_token = tokens[0, i].item()\n",
        "                next_score = score + log_probs[0, i].item()\n",
        "                candidate = (next_score, trg_indexes + [next_token])\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "        if any([cand[1][-1] == vocab_tgt['<eos>'] for cand in beams]):\n",
        "            break\n",
        "\n",
        "    best_beam = beams[0][1]\n",
        "    translated_sentence = ' '.join([vocab_tgt.lookup_token(i) for i in best_beam[1:]])\n",
        "\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "nD3C2dJiC_Oh"
      },
      "id": "nD3C2dJiC_Oh",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoXURLczDI8o",
        "outputId": "2a29c9f0-1b8f-40e0-8709-0fa3a7fb49bf"
      },
      "id": "EoXURLczDI8o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A young boy is playing in a pool . <eos>\n",
            "Translated sentence: A man with glasses and a beard is sitting on a bench in front of a microphone\n",
            "Translated sentence: A crowd of people are dancing . <eos>\n",
            "Translated sentence: Construction workers are working on a street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dNPqQ9aANY2r"
      },
      "id": "dNPqQ9aANY2r"
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed hyperparameters\n",
        "d_model = 768  # Embedding dimension\n",
        "N = 7          # Number of encoder and decoder layers\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUwTfLVGHNGD",
        "outputId": "8f01797c-7c73-4701-eabd-5da1750293e7"
      },
      "id": "RUwTfLVGHNGD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.695\n",
            "\tVal Loss: 5.197\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.165\n",
            "\tVal Loss: 5.162\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 5.113\n",
            "\tVal Loss: 5.203\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 5.086\n",
            "\tVal Loss: 5.204\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 5.122\n",
            "\tVal Loss: 5.249\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 5.059\n",
            "\tVal Loss: 5.098\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 4.879\n",
            "\tVal Loss: 5.346\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 4.786\n",
            "\tVal Loss: 5.658\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 4.739\n",
            "\tVal Loss: 5.793\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 4.704\n",
            "\tVal Loss: 5.977\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 4.679\n",
            "\tVal Loss: 6.049\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 4.652\n",
            "\tVal Loss: 6.102\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 4.630\n",
            "\tVal Loss: 6.190\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 4.610\n",
            "\tVal Loss: 6.392\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 4.590\n",
            "\tVal Loss: 6.509\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 4.571\n",
            "\tVal Loss: 6.378\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 4.744\n",
            "\tVal Loss: 5.267\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 4.813\n",
            "\tVal Loss: 11.840\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 4.692\n",
            "\tVal Loss: 12.878\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 4.586\n",
            "\tVal Loss: 14.489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXUCim7nMJlj",
        "outputId": "33dce9f1-e7d0-4c33-8644-e37e2c873400"
      },
      "id": "zXUCim7nMJlj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replicating GPT2 with hyperparameters"
      ],
      "metadata": {
        "id": "mdsyjrm-Bflk"
      },
      "id": "mdsyjrm-Bflk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 768   # Embedding dimension to match GPT-2 small\n",
        "N = 12          # Number of layers; GPT-2 is decoder-only, but for a seq2seq model, this could apply to each part\n",
        "num_heads = 12  # Number of attention heads to match GPT-2 small\n",
        "d_ff = 3072     # Dimension of feed-forward networks, approximated to GPT-2's configuration\n",
        "max_seq_length = 1024 # Adjusted maximum sequence length to match GPT-2's capability\n",
        "dropout = 0.3  # Dropout rate increased to handle overfitting\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters for the training process\n",
        "batch_size = 512\n",
        "grad_clip = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHAo8_CIAo80",
        "outputId": "1c2b5d08-7923-457c-c14b-6f6ca3254980"
      },
      "id": "MHAo8_CIAo80",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 7.296\n",
            "\tVal Loss: 6.213\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.855\n",
            "\tVal Loss: 6.394\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 5.403\n",
            "\tVal Loss: 7.253\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 5.281\n",
            "\tVal Loss: 7.658\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 5.208\n",
            "\tVal Loss: 7.906\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 5.152\n",
            "\tVal Loss: 7.810\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 5.104\n",
            "\tVal Loss: 6.784\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 5.071\n",
            "\tVal Loss: 6.509\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 5.050\n",
            "\tVal Loss: 6.067\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 5.036\n",
            "\tVal Loss: 5.880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRA2JVIKHx4Z",
        "outputId": "82d898df-c1dc-4b36-f02d-ab622e3f96a0"
      },
      "id": "CRA2JVIKHx4Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A a . <eos>\n",
            "Translated sentence: A in a a a a a .\n",
            "Translated sentence: A .\n",
            "Translated sentence: A .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 10 more epochs (total 20 epochs)\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US9ajc0pXLfY",
        "outputId": "507a7a60-2732-406f-8fb1-d4b112925ad7"
      },
      "id": "US9ajc0pXLfY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.022\n",
            "\tVal Loss: 5.463\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.010\n",
            "\tVal Loss: 5.220\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.998\n",
            "\tVal Loss: 5.208\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.988\n",
            "\tVal Loss: 5.089\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.979\n",
            "\tVal Loss: 5.097\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 4.970\n",
            "\tVal Loss: 5.109\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 4.963\n",
            "\tVal Loss: 5.132\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 4.955\n",
            "\tVal Loss: 5.145\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 4.949\n",
            "\tVal Loss: 5.253\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 4.946\n",
            "\tVal Loss: 5.289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence_beam_search(model, src_sentence, vocab_src, vocab_tgt, beam_width=10)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlCv_3cbXP06",
        "outputId": "ed693eb3-c232-4b51-ea97-3f0cb638f739"
      },
      "id": "BlCv_3cbXP06",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A man in in a a a a a a a . .\n",
            "Translated sentence: A man in in a a a a a a a a a a a a a a .\n",
            "Translated sentence: A are are are the .\n",
            "Translated sentence: A are are are a . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Increasing number of attention heads"
      ],
      "metadata": {
        "id": "PhSXzwtE6JPr"
      },
      "id": "PhSXzwtE6JPr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed hyperparameters\n",
        "d_model = 528  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 12  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qUUcRKBmLpO",
        "outputId": "fb8216d7-b0b7-43dc-adda-79d7278f78c0"
      },
      "id": "2qUUcRKBmLpO",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.706\n",
            "\tVal Loss: 5.010\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.893\n",
            "\tVal Loss: 4.763\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.675\n",
            "\tVal Loss: 4.612\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.397\n",
            "\tVal Loss: 4.237\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.074\n",
            "\tVal Loss: 3.989\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.873\n",
            "\tVal Loss: 3.872\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.740\n",
            "\tVal Loss: 3.766\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.618\n",
            "\tVal Loss: 3.676\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.520\n",
            "\tVal Loss: 3.613\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.442\n",
            "\tVal Loss: 3.561\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.371\n",
            "\tVal Loss: 3.507\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.309\n",
            "\tVal Loss: 3.467\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.241\n",
            "\tVal Loss: 3.417\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.178\n",
            "\tVal Loss: 3.370\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.115\n",
            "\tVal Loss: 3.319\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.060\n",
            "\tVal Loss: 3.287\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.007\n",
            "\tVal Loss: 3.249\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.956\n",
            "\tVal Loss: 3.211\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.904\n",
            "\tVal Loss: 3.187\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.858\n",
            "\tVal Loss: 3.172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ein Mann in einem Anzug steht auf einer Bühne und spricht in ein Mikrofon.\"  # A man in a suit stands on a stage and speaks into a microphone\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Ich liebe indisches Essen.\" # I love Indian food.\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n",
        "\n",
        "src_sentence = \"Wer übernachtet in der Snell-Bibliothek?\" # Who stays at night in Snell library?\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrN22MVf3Kdp",
        "outputId": "f51641e4-3acd-4d2d-c561-9e1a4d3e6cc7"
      },
      "id": "IrN22MVf3Kdp",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A young boy is playing in a pool . <eos>\n",
            "Translated sentence: A man and a woman are sitting in front of a woman in a white shirt . <eos>\n",
            "Translated sentence: Construction workers are working at a party . <eos>\n",
            "Translated sentence: Construction workers are working on the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA2a0Qk14vZN"
      },
      "id": "wA2a0Qk14vZN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}