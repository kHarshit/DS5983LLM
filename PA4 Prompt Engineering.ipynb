{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e930bb25",
   "metadata": {
    "id": "e930bb25"
   },
   "source": [
    "In this exercise, you will perform prompt engineering on a dialogue summarization task using [Flan-T5](https://huggingface.co/google/flan-t5-large) and the [dialogsum dataset](https://huggingface.co/datasets/knkarthick/dialogsum). You will explore how different prompts affect the output of the model, and compare zero-shot and few-shot inferences. <br/>\n",
    "Complete the code in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e35b68",
   "metadata": {
    "id": "70e35b68"
   },
   "source": [
    "### 1. Set up Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e6529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "676e6529",
    "outputId": "b05f66fd-b01d-448a-a55a-b6a41a9e56e0"
   },
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfaae5",
   "metadata": {
    "id": "fddfaae5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OH-HZBXf5YDl",
   "metadata": {
    "id": "OH-HZBXf5YDl"
   },
   "source": [
    "### 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247b70a",
   "metadata": {
    "id": "5247b70a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('knkarthick/dialogsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c131e",
   "metadata": {
    "id": "854c131e"
   },
   "source": [
    "Print several dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8e4d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65a8e4d3",
    "outputId": "eb4fac97-85c4-4667-d539-609630532804"
   },
   "outputs": [],
   "source": [
    "example_indices = [0, 42, 800]\n",
    "dash_line = '-' * 100\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd15699",
   "metadata": {
    "id": "bdd15699"
   },
   "source": [
    "### 3. Summarize Dialogues without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f575c6f",
   "metadata": {
    "id": "4f575c6f"
   },
   "source": [
    "Load the Flan-T5-large model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7777e",
   "metadata": {
    "id": "cde7777e"
   },
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-large'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eb814",
   "metadata": {
    "id": "1e9eb814"
   },
   "source": [
    "**Exercise**: Use the pre-trained model to summarize the example dialogues without any prompt engineering. Use the `model.generate()` function with `max_new_tokens=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc5c4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1acc5c4c",
    "outputId": "345301d9-2882-413b-9607-1c91a5b4712b"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c1847",
   "metadata": {
    "id": "f56c1847"
   },
   "source": [
    "You can see that the model generations make some sense, but the model doesn't seem to be sure what task it is supposed to accomplish and it often just makes up the next sentence in the dialogue. Prompt engineering can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d357a5e",
   "metadata": {
    "id": "3d357a5e"
   },
   "source": [
    "### 4. Summarize Dialogues with Instruction Prompts\n",
    "\n",
    "In order to instruct the model to perform a task (e.g., summarize a dialogue), you can take the dialogue and convert it into an instruction prompt. This is often called **zero-shot inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f063fe5",
   "metadata": {
    "id": "9f063fe5"
   },
   "source": [
    "**Exercise**: Wrap the dialogues in a descriptive instruction (e.g., \"Summarize the following conversation.\"), and examine how the generated text changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce562836",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce562836",
    "outputId": "c4a63a8d-f9c7-4c8f-d512-71e139c9f6c8"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84813",
   "metadata": {
    "id": "b3f84813"
   },
   "source": [
    "This is much better! But the model still does not pick up on the nuance of the conversations though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35a918",
   "metadata": {
    "id": "fd35a918"
   },
   "source": [
    "**Exercise:** Experiment with the prompt text and see how it influences the generated output. Do the inferences change if you end the prompt with just empty string vs. `Summary: `?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tzzxiWsM7Mzd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzzxiWsM7Mzd",
    "outputId": "e1cbef14-180d-476e-e634-eaa8dceb3215"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4c1fd",
   "metadata": {
    "id": "75f4c1fd"
   },
   "source": [
    "**Exercise:** Flan-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py). Try using its pre-built prompts for dialogue summarization (e.g., the ones under the `\"samsum\"` key) and see how they influence the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef476f6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef476f6d",
    "outputId": "da91f986-ca25-4263-f431-940d6ccf7d75"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85043960",
   "metadata": {
    "id": "85043960"
   },
   "source": [
    "Notice that the prompts from Flan-T5 did help, but the model still struggles to pick up on the nuance of the conversation in some cases. This is what you will try to solve with few-shot inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca21d4c",
   "metadata": {
    "id": "aca21d4c"
   },
   "source": [
    "### 5. Summarize Dialogues with a Few-Shot Inference\n",
    "\n",
    "**Few-shot inference** is the practice of providing an LLM with several examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2552a",
   "metadata": {
    "id": "1df2552a"
   },
   "source": [
    "**Exercise:** Build a function that takes a list of `in_context_example_indexes`, generates a prompt with the examples, then at the end appends the prompt that you want the model to complete (`test_example_index`). Use the same Flan-T5 prompt template from Section 3. Make sure to separate between the examples with `\"\\n\\n\\n\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b8c38",
   "metadata": {
    "id": "130b8c38"
   },
   "outputs": [],
   "source": [
    "def make_prompt(in_context_example_indices, test_example_index):\n",
    "    ### WRITE YOUR CODE HERE \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "         \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968a7e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e968a7e1",
    "outputId": "6977ede2-791a-4f80-faaa-c9a99fadb060"
   },
   "outputs": [],
   "source": [
    "in_context_example_indices = [0, 10, 20]\n",
    "test_example_index = 800\n",
    "\n",
    "few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1afcfc",
   "metadata": {
    "id": "bb1afcfc"
   },
   "source": [
    "Now pass this prompt to the model perform a few shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989f7ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f989f7ef",
    "outputId": "079bf58f-c5ea-4434-d0d3-d26d86e332d4"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588bf0c",
   "metadata": {
    "id": "e588bf0c"
   },
   "source": [
    "**Exercise:** Experiment with the few-shot inferencing:\n",
    "- Choose different dialogues - change the indices in the `in_context_example_indices` list and `test_example_index` value.\n",
    "- Change the number of examples. Be sure to stay within the model's 512 context length, however.\n",
    "\n",
    "How well does few-shot inference work with other examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99b9cf",
   "metadata": {
    "id": "fd99b9cf"
   },
   "source": [
    "### 6. Generative Configuration Parameters for Inference\n",
    "\n",
    "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. By setting the parameter `do_sample = True`, you can activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a66a1",
   "metadata": {
    "id": "6a2a66a1"
   },
   "source": [
    "**Exercise:** Change the configuration parameters to investigate their influence on the output. Analyze your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c7c65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "487c7c65",
    "outputId": "521f0ab7-6eef-4267-b59c-4dec619d7b1c"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
